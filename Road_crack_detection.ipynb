{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs4imnWJDRKH"
      },
      "source": [
        "Author: Hari Kumar<br>\n",
        "Created Date: 10/17/2022\n",
        "\n",
        "*These Project helps in detectin of pothholes or anomoulies in the road.Through the machine learning image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhdl56ZMDZt3"
      },
      "source": [
        "Approach\n",
        "\n",
        "In india there are serveral accidents due to pothholes and road anomolies.These difficult to Report about the potholes or assess it without a manual survey.So to solve these issue we will be training a model that detects cracks and potholes on the roads.\n",
        "The images collected are by using drones and also can be sent by people to a server with location specification so as to.The Large scale images are reduced and processed accordingly using image Features.\n",
        "\n",
        "There will be two main data sets.One is Pothole and other is not_pothole.We will introduce the non_pothole and pothole images as 85 %training dataset and later we will use rest 15% for evaluation the model.\n",
        "And Once the model is deployed the people can upload the images which will increase the Precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRy5fJt6JRiV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jeF3nWM1V6a"
      },
      "source": [
        "All the Neccessary Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzmP5xQkQXs7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from PIL import Image\n",
        "from sklearn.calibration import calibration_curve\n",
        "import os, requests, zipfile\n",
        "!pip install GitPython\n",
        "from git import Repo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "617ACBNK11lg"
      },
      "source": [
        "Data Set Contains Three Directories:\n",
        "\n",
        "\n",
        "1. Train/\n",
        "   a. Crack\n",
        "   b. non_cracl\n",
        "2. Test/\n",
        "   a. Crack\n",
        "   b.non_crack\n",
        "3. Val/\n",
        "   a.Crack\n",
        "   b.non_crack\n",
        "\n",
        "The downloaded dat will be stored on the in source directory. As temp_concrete_crack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NQSA-ky2EQh"
      },
      "source": [
        "Each batch will have 32 Pics\n",
        "And Photo Dimensions will be 227 * 277 Px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBRS4392W3p"
      },
      "source": [
        "As of Now we will be using dataset of https://github.com/Hari-maloth/Road-Crack-Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj7lDMEac_P8"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "photo_height = 227\n",
        "photo_width = 227\n",
        "\n",
        "filepath = 'road_cracks'\n",
        "\n",
        "try:\n",
        "    Repo.clone_from('https://github.com/Hari-maloth/Road-Crack-Detection.git', filepath)\n",
        "except:\n",
        "    PRINT('YES')\n",
        "    pass\n",
        "\n",
        "#AUTOTUNE = tf.d\n",
        "\n",
        "#train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "#val = val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6hykWxv2hvJ"
      },
      "outputs": [],
      "source": [
        "#Assaining file paths.\n",
        "train_path = r'{road_cracks}/dataset/train/'.format(road_cracks=filepath)\n",
        "test_path = r'{road_cracks}/dataset/test'.format(road_cracks=filepath)\n",
        "val_path = r'{road_cracks}/dataset/val'.format(road_cracks=filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIlH6bka2n_B"
      },
      "outputs": [],
      "source": [
        "#Training dataset with train_path(Pre Processing).\n",
        "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_path,\n",
        "  validation_split=None,\n",
        "  subset=None,\n",
        "  seed=42,\n",
        "  image_size=(photo_height, photo_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVU4tVYe23no"
      },
      "outputs": [],
      "source": [
        "#Validation\n",
        "\n",
        "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  val_path,\n",
        "  validation_split=None,\n",
        "  subset=None,\n",
        "  seed=42,\n",
        "  image_size=(photo_height, photo_width),\n",
        "  batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAdHdnsG29aw"
      },
      "outputs": [],
      "source": [
        "#Assaining Classes Names for datasets.\n",
        "classes = train.class_names\n",
        "print('classes: ' +str(classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxxyAFurc_P8"
      },
      "source": [
        "#Preview of Data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10uA0YxT3R1R"
      },
      "outputs": [],
      "source": [
        "#Number of Pics You want Visulasize\n",
        "a=int(input())\n",
        "#Size of the Pic You want to Visualize\n",
        "b=int(input())\n",
        "if b>=32:\n",
        "  print('Please Rerun this cell Also Enter B Value Below 32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jgWa_eKc_P8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(b, b))\n",
        "for images, labels in train.take(1):\n",
        "  for i in range(a):\n",
        "    ax = plt.subplot((a/4), 4, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(classes[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdOJ7q7jc_P9"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#There are two classes One is Crack Other not_Crack\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#There will a preprocessing layer first it scale all the images in data set to a size of 255 px \n",
        "#After that there will be there relu activation layers\n",
        "model = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "    \n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "print('compiled!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBfmXdmjc_P9"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3)\n",
        "\n",
        "#We will have 20 epochs for the dataset.\n",
        "history = model.fit(\n",
        "  train,\n",
        "  validation_data=val,\n",
        "  epochs=20,\n",
        "    callbacks=[model_checkpoint, early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "h = pd.DataFrame(history.history)\n",
        "h['epoch'] = h.index + 1\n",
        "\n",
        "#This is not necessary in the model for as notebook to understand and visualize the data set we will matplolib graphs\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(h['epoch'], h['accuracy'], h['epoch'], h['val_accuracy'])\n",
        "plt.title('Model History')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend(('train accuracy', 'val accuracy'),\n",
        "           loc='lower right')\n",
        "ax1 = plt.gca()\n",
        "ax1.set_xticks(range(1, max(h['epoch'])+1))\n",
        "\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(h['epoch'], h['loss'], h['epoch'], h['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.legend(('train loss', 'val loss'),\n",
        "           loc='upper right')\n",
        "ax2 = plt.gca()\n",
        "ax2.set_xticks(range(1, max(h['epoch'])+1))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#model.save('temp_concrete_crack')\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ew4gElec_P9"
      },
      "source": [
        "## Make Predictions\n",
        "\n",
        "The following cell pulls all of the test photos, converts them to arrays, gets a prediction for them, and appends the results from each photo in a dataframe. You can use the import method from above as well, but it can be a bit difficult to get the results a useful format.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncSBqJ7zc_P-"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame(columns=['predicted', 'actual', 'probability', 'file', 'raw_logits'])\n",
        "\n",
        "at = 0\n",
        "#Giving filepaths to labels .\n",
        "# For Validation Image Set with 4000 Images are used along with labels.\n",
        "for label in classes: \n",
        "    path = test_path+'/'+label\n",
        "    print(path)\n",
        "    for file in glob.iglob(path + '/*'):\n",
        "        img = keras.preprocessing.image.load_img(\n",
        "        file, target_size=(photo_height, photo_width)\n",
        "        )\n",
        "        \n",
        "        img_array = keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "        \n",
        "        predictions = loaded_model.predict(img_array)\n",
        "        score = tf.nn.softmax(predictions[0])\n",
        "        results = results.append({'predicted': classes[np.argmax(score)], 'actual': label, \n",
        "                                  'probability': 100 * np.max(score), 'file': file,\n",
        "                                 'raw_logits': score}, ignore_index=True)\n",
        "        at +=1\n",
        "        if at % 200 == 0:\n",
        "            print(str(round(((at / 4000) * 100), 1))+'%')\n",
        "\n",
        "        \n",
        "\n",
        "score = accuracy_score(results['actual'], results['predicted'])\n",
        "print('Test Accuracy : '+str(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cCkMS3jc_P-"
      },
      "source": [
        "## Incorect Predictions\n",
        "We have got accuarcy over 99 percent so the errors in prediction are little.But it is Necessary to Understand why did it happen.\n",
        "So lets visualize the Data set of images that got Wrong prediction while the model is processing ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Get values for incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hv-xcfJc_P-"
      },
      "outputs": [],
      "source": [
        "for i in classes:\n",
        "   incorrect = results.loc[(results['predicted'] != i) & (results['actual'] == i)]\n",
        "   print(incorrect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Get\n",
        " Details For Correct Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in classes:\n",
        "    correct=results.loc[(results['predicted']!=1)&(results['actual']==i)]\n",
        "    print(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTYtap_dGC_r"
      },
      "source": [
        "We can see that non clear and blur images got less probability of cracks mostly. And also when image disturbance is Big."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nunKW8NUGW8j"
      },
      "source": [
        "Lets Take a look probability distribution for incorrect classifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VFGkMmZGeTE"
      },
      "source": [
        "We will Use Pandas to Understand the errors in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIkCSOpoc_P_"
      },
      "outputs": [],
      "source": [
        "crack = results.loc[(results['predicted'] == 'crack')]\n",
        "no_crack = results.loc[(results['predicted'] == 'no_crack')]\n",
        "probs = pd.DataFrame({'Crack': crack['probability'], 'No Crack': no_crack['probability']})\n",
        "\n",
        "ax = probs.plot.kde(ind=[i for i in np.linspace(start = 60, stop = 110, num = 1000)], \n",
        "                    title='Probability Distribution', xlabel='Probability')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21DkcD5E9fBL"
      },
      "source": [
        "We can see model is more accurate for most images for crack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPkaggNfc_QA"
      },
      "outputs": [],
      "source": [
        "prob_no_crack = []\n",
        "for i in range(len(results)):\n",
        "    logits = results['raw_logits'][i]\n",
        "    prob_no_crack.append(logits[1])\n",
        "\n",
        "prob_true_binary, prob_pred_binary = calibration_curve(\n",
        "        results['actual'].map({'crack': 0, 'no_crack': 1}), \n",
        "        prob_no_crack, n_bins=4, normalize=False)\n",
        "\n",
        "#We will use matplotlib to visualize the graph.\n",
        "fig = plt.figure()\n",
        "ax = plt.gca()\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='tab:pink', linestyle=\":\", label=\"Calibrated Model\")\n",
        "plt.plot(prob_pred_binary, prob_true_binary, label='Classifier', color=\"tab:grey\")\n",
        "\n",
        "plt.ylabel('positive rate')\n",
        "plt.xlabel('predicted value')\n",
        "\n",
        "plt.legend()\n",
        "plt.yticks()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4dPsAvZ9fBM"
      },
      "source": [
        "Calibrated model performs better than classifier model.And its variance between variable is 1.Which means it is good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-fn7EUHIAs8"
      },
      "source": [
        "This Last line will remove downloaded data so Please Enter after all the evaluations are done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDOU2Q6d9fBM"
      },
      "outputs": [],
      "source": [
        "# delete all files downloaded in this notebook\n",
        "import datetime\n",
        "from git import rmtree\n",
        "print('Type Yes or Y or y or yes to delete the downloaded resources')\n",
        "user=input()\n",
        "if user==\"Yes\" or user=='yes' or user=='y' or user==\"Y\":\n",
        "  print('The downloaded resource files are deleted.Deletion intiated at '+str(datetime.datetime.now())+\" UTC Time     Thank You for Presence.\")\n",
        "  rmtree('./'+filepath)\n",
        "  print('The downloaded resource files are deleted.At '+str(time())+\"Thank You for Presence.\")\n",
        "else:\n",
        "    print(\"Waiting for input to Delete.If you want to delete the downloaded resources input y or Yes or yes or Y\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3apLWQ0c_QA"
      },
      "source": [
        "#Arguments\n",
        "\n",
        "\n",
        "The model Generated will help classifing cracks with no cracks.Similar model can introduced for pothole and no pothole as well.There are several limations with computer vision problem such as image should be clear and model should be trained with clear images."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf38",
      "language": "python",
      "name": "tf38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
